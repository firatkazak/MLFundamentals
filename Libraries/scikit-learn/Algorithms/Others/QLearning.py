import numpy as np
import gym

# Ã‡evreyi oluÅŸturun
env = gym.make('CartPole-v1')

# Q-Ã¶ÄŸrenme iÃ§in parametreler
n_episodes = 1000  # Toplam epizod sayÄ±sÄ±
if isinstance(env.action_space, gym.spaces.Discrete):
    n_actions = env.action_space.n  # Aksiyon sayÄ±sÄ±
else:
    raise ValueError("Aksiyon alanÄ± Discrete deÄŸil.")

# Durum sayÄ±sÄ±nÄ± belirlemek iÃ§in CartPole iÃ§in durumlarÄ± ayrÄ±ÅŸtÄ±rmamÄ±z gerekiyor
n_states = 10  # Bu Ã¶rnekte 10 ayrÄ± duruma ayrÄ±ÅŸtÄ±rÄ±yoruz

# Q tablosunu baÅŸlatÄ±n
Q = np.zeros((n_states, n_actions))

# Hiperparametreler
alpha = 0.1  # Ã–ÄŸrenme oranÄ±
gamma = 0.99  # Ä°ndirim oranÄ±
epsilon = 1.0  # KeÅŸif oranÄ±
epsilon_decay = 0.995  # Epsilon'un azalmasÄ±


def discretize(cartpole_state):
    # Durumu ayrÄ±ÅŸtÄ±r (discretization)
    return int(np.digitize(cartpole_state[2], np.linspace(-2.0, 2.0, n_states)))


# Q-Ã¶ÄŸrenme dÃ¶ngÃ¼sÃ¼
for episode in range(n_episodes):
    state = env.reset()  # State burada 4 elemanlÄ± bir dizi olmalÄ±

    # EÄŸer state bir tuple ise, ilk elemanÄ± al
    if isinstance(state, tuple):
        state = state[0]  # Tuple'dan durumu al

    print(f"Initial state: {state}")  # BaÅŸlangÄ±Ã§ durumunu yazdÄ±r
    done = False

    while not done:
        state_index = discretize(state)  # state_index burada tanÄ±mlanmalÄ±

        # Epsilon-greedy stratejisi ile aksiyon seÃ§
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # Rastgele aksiyon
        else:
            action = np.argmax(Q[state_index])  # En iyi aksiyonu seÃ§

        # Aksiyonu uygula ve geri bildirim al
        next_state, reward, done, truncated, _ = env.step(action)  # DÃ¶rdÃ¼ncÃ¼ deÄŸer (truncated) eklendi

        # Durumu gÃ¼ncelle ve ayrÄ±ÅŸtÄ±r
        if isinstance(next_state, tuple):
            next_state = next_state[0]  # Tuple'dan durumu al

        next_state_index = discretize(next_state)

        # Q deÄŸerini gÃ¼ncelle
        Q[state_index, action] += alpha * (reward + gamma * np.max(Q[next_state_index]) - Q[state_index, action])

        # Durumu gÃ¼ncelle
        state = next_state

    # Epsilon'u gÃ¼ncelle
    epsilon *= epsilon_decay

# Ã‡evreyi kapat
env.close()

# Q-Learning:
# Q-Learning, pekiÅŸtirmeli Ã¶ÄŸrenmenin temel yÃ¶ntemlerinden biridir ve makine Ã¶ÄŸrenmesinin derinliklerine inmenizi saÄŸlar.
# Temel ilkeleri kavrayarak, daha karmaÅŸÄ±k pekiÅŸtirmeli Ã¶ÄŸrenme algoritmalarÄ±nÄ± anlamak iÃ§in saÄŸlam bir temel oluÅŸturursunuz.
# Her aÅŸamada deneyim kazanarak, bu kavramlarÄ± daha iyi anlayabilir ve uygulamalarÄ±nÄ±zda daha etkili sonuÃ§lar elde edebilirsiniz.

# 1. Temel Kavramlar
# Markov Karar SÃ¼reÃ§leri (MDP): Q-Learning, karar verme problemlerini Ã§Ã¶zmek iÃ§in kullanÄ±lan bir yÃ¶ntemdir.
# Markov Karar SÃ¼reÃ§leri, belirli bir durumdan baÅŸlayarak gelecekteki durumlarÄ± ve Ã¶dÃ¼lleri tahmin etmek iÃ§in kullanÄ±lÄ±r.
#
# Durum (State): OrtamÄ±n mevcut durumu. Ã–rneÄŸin, CartPole'da, kutunun konumu ve aÃ§Ä±sÄ± gibi faktÃ¶rler durumlarÄ± belirler.
#
# Aksiyon (Action): AlgoritmanÄ±n belirli bir durumda alabileceÄŸi eylemler. Ã–rneÄŸin, CartPole'da kutuyu sola veya saÄŸa hareket ettirmek gibi.
#
# Ã–dÃ¼l (Reward): Bir aksiyonun sonucunda alÄ±nan geri bildirim. Q-Learning'de, Ã¶dÃ¼l, ajanÄ±nÄ±zÄ±n ortamda ne kadar baÅŸarÄ±lÄ± olduÄŸunu belirler.
#
# 2. Q-Tablosu
# Q-DeÄŸerleri: Her bir durum ve aksiyon Ã§ifti iÃ§in beklenen Ã¶dÃ¼lÃ¼ temsil eden deÄŸerlerdir.
# Q-Tablosu, bu deÄŸerleri depolar. Q-DeÄŸerlerini gÃ¼ncelleyerek, ajanÄ±nÄ±zÄ±n Ã¶ÄŸrenme sÃ¼recini yÃ¶nlendirirsiniz.
# 3. Ã–ÄŸrenme SÃ¼reci
# Epsilon-Greedy Stratejisi: KeÅŸif (exploration) ve istismar (exploitation) arasÄ±nda denge kurar.
# BaÅŸlangÄ±Ã§ta yÃ¼ksek bir keÅŸif oranÄ± ile rastgele eylemler seÃ§erken, zamanla daha iyi bilgilere sahip oldukÃ§a en iyi eylemi seÃ§me olasÄ±lÄ±ÄŸÄ± artar.
#
# Q-DeÄŸer GÃ¼ncellemesi: Q-DeÄŸerlerini gÃ¼ncelleyerek, ajanÄ±nÄ±zÄ±n geÃ§miÅŸ deneyimlerinden Ã¶ÄŸrenmesini saÄŸlarsÄ±nÄ±z.

# 4. Pratik Uygulamalar
# Ã–ÄŸrenme OranÄ± (
# ğ›¼
# Î±) ve Ä°ndirim OranÄ± (
# ğ›¾
# Î³): Bu iki parametre, Ã¶ÄŸrenme sÃ¼recinizin nasÄ±l gideceÄŸini belirler. FarklÄ± ayarlarla deneyler yaparak hangi ayarlarÄ±n en iyi sonucu verdiÄŸini gÃ¶zlemleyebilirsiniz.
#
# Daha KarmaÅŸÄ±k Ortamlar: CartPole basit bir Ã¶rnektir. Q-Learning'i daha karmaÅŸÄ±k ortamlarla deneyerek, daha fazla deneyim kazanabilirsiniz.

# 5. GeliÅŸmiÅŸ YÃ¶ntemler
# Derin Q-AÄŸlarÄ± (DQN): Q-Learningâ€™in derin Ã¶ÄŸrenme ile birleÅŸtirilmiÅŸ hali, daha karmaÅŸÄ±k durumlarÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lÄ±r.
#
# Double Q-Learning: AÅŸÄ±rÄ± tahminleri Ã¶nlemek iÃ§in iki ayrÄ± Q-Tablosu kullanÄ±r.
#
# Prioritized Experience Replay: Deneyimlerin Ã¶nceliklendirilmesi ile Ã¶ÄŸrenme sÃ¼recini iyileÅŸtirir.
